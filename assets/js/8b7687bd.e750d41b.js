"use strict";(self.webpackChunk_indeepvision_visioncloud_docs=self.webpackChunk_indeepvision_visioncloud_docs||[]).push([[559],{2247:(e,t,n)=>{n.d(t,{xA:()=>u,yg:()=>p});var l=n(4041);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);t&&(l=l.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,l)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,l,a=function(e,t){if(null==e)return{};var n,l,a={},r=Object.keys(e);for(l=0;l<r.length;l++)n=r[l],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(l=0;l<r.length;l++)n=r[l],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=l.createContext({}),g=function(e){var t=l.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=g(e.components);return l.createElement(s.Provider,{value:t},e.children)},y="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return l.createElement(l.Fragment,{},t)}},c=l.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),y=g(n),c=a,p=y["".concat(s,".").concat(c)]||y[c]||d[c]||r;return n?l.createElement(p,i(i({ref:t},u),{},{components:n})):l.createElement(p,i({ref:t},u))}));function p(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=c;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[y]="string"==typeof e?e:a,i[1]=o;for(var g=2;g<r;g++)i[g]=n[g];return l.createElement.apply(null,i)}return l.createElement.apply(null,n)}c.displayName="MDXCreateElement"},8288:(e,t,n)=>{n.d(t,{A:()=>l});const l=n.p+"assets/images/8-e8a12821bd664819cc041769cfc6915e.png"},8744:(e,t,n)=>{n.d(t,{A:()=>l});const l=n.p+"assets/images/model_parameters-7371807460dd29e74742954c87e3c2ed.png"},8748:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>r,metadata:()=>o,toc:()=>g});var l=n(9575),a=(n(4041),n(2247));const r={sidebar_position:3},i="Advanced Configuration",o={unversionedId:"user-guide/models/advanced-configuration",id:"user-guide/models/advanced-configuration",title:"Advanced Configuration",description:"The _Parameters_ section is your control center for configuring the deep learning architecture and training process. Making the right parameter choices can dramatically impact training time, model accuracy, and real-world performance.",source:"@site/docs/user-guide/models/advanced-configuration.md",sourceDirName:"user-guide/models",slug:"/user-guide/models/advanced-configuration",permalink:"/visioncloud/docs/user-guide/models/advanced-configuration",draft:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"userGuide",previous:{title:"Add Datasets",permalink:"/visioncloud/docs/user-guide/models/add-datasets"},next:{title:"Start Training",permalink:"/visioncloud/docs/user-guide/models/start-training"}},s={},g=[{value:"Model Architecture",id:"model-architecture",level:2},{value:"Backbone Selection",id:"backbone-selection",level:3},{value:"Backbone Size",id:"backbone-size",level:3},{value:"Output Downsample",id:"output-downsample",level:3},{value:"Loss configuration",id:"loss-configuration",level:3},{value:"Dataset Parameters",id:"dataset-parameters",level:2},{value:"Data Augmentation",id:"data-augmentation",level:3},{value:"Training Parameters",id:"training-parameters",level:2},{value:"Learning rate",id:"learning-rate",level:3},{value:"Batch Size",id:"batch-size",level:4},{value:"Epochs",id:"epochs",level:4},{value:"Early Stopping",id:"early-stopping",level:4},{value:"Additional Training Settings",id:"additional-training-settings",level:3},{value:"Class Balancing",id:"class-balancing",level:4},{value:"Mixed Precision Training",id:"mixed-precision-training",level:4}],u={toc:g},y="wrapper";function d(e){let{components:t,...r}=e;return(0,a.yg)(y,(0,l.A)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"advanced-configuration"},"Advanced Configuration"),(0,a.yg)("p",null,"The ",(0,a.yg)("strong",{parentName:"p"},(0,a.yg)("em",{parentName:"strong"},"Parameters"))," section is your control center for configuring the deep learning architecture and training process. Making the right parameter choices can dramatically impact training time, model accuracy, and real-world performance."),(0,a.yg)("p",null,"Parameters are grouped into three main categories to help you navigate the many options available:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Model Architecture"),": Define the neural network structure and behavior"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Dataset Parameters"),": Control how your data is processed and augmented"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Training Parameters"),": Configure the optimization process")),(0,a.yg)("p",null,"Each parameter category affects different aspects of your model's performance and behavior."),(0,a.yg)("figure",null,(0,a.yg)("img",{src:n(8744).A,alt:"Model parameters"}),(0,a.yg)("figcaption",null,"Parameters section with the three main configuration categories.")),(0,a.yg)("h2",{id:"model-architecture"},"Model Architecture"),(0,a.yg)("p",null,"Define the fundamental structure of your neural network. These choices affect capacity, speed, and learning ability."),(0,a.yg)("h3",{id:"backbone-selection"},"Backbone Selection"),(0,a.yg)("p",null,"The backbone is the pre-trained neural network that serves as the foundation for your model. Different backbones offer different tradeoffs between accuracy and computational efficiency:"),(0,a.yg)("div",{style:{display:"flex",justifyContent:"center"}},(0,a.yg)("table",null,(0,a.yg)("thead",null,(0,a.yg)("tr",null,(0,a.yg)("th",null,"Backbone Family"),(0,a.yg)("th",null,"Characteristics"),(0,a.yg)("th",null,"Best For"))),(0,a.yg)("tbody",null,(0,a.yg)("tr",null,(0,a.yg)("td",null,"Simple"),(0,a.yg)("td",null,"Simple and fast, but limited in capacity"),(0,a.yg)("td",null,"Basic classification tasks")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"ResNet-Simple"),(0,a.yg)("td",null,"Custom implementation of ResNet"),(0,a.yg)("td",null,"Complex classification tasks")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"ResNet"),(0,a.yg)("td",null,"Good feature extraction, residual connections prevent vanishing gradients"),(0,a.yg)("td",null,"General purpose, excellent starting point for most tasks")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"VGG"),(0,a.yg)("td",null,"Simple architecture, strong feature extraction"),(0,a.yg)("td",null,"Tasks requiring fine-grained visual features")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"DenseNet"),(0,a.yg)("td",null,"Dense connections, parameter efficient"),(0,a.yg)("td",null,"Complex visual tasks with limited training data")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"EfficientNet"),(0,a.yg)("td",null,"Optimized for efficiency, scales well across different sizes"),(0,a.yg)("td",null,"Mobile applications, resource-constrained environments")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Densenet"),(0,a.yg)("td",null,"Dense connections, parameter efficient"),(0,a.yg)("td",null,"Complex visual tasks with limited training data"))))),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Decision Guide:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"First model? \u2192 ResNet-Simple\nLimited resources? \u2192 EfficientNetB0\nComplex task + lots of data? \u2192 ResNet50 or DenseNet\nNeed fine details? \u2192 VGG\n")),(0,a.yg)("admonition",{type:"tip"},(0,a.yg)("mdxAdmonitionTitle",{parentName:"admonition"},(0,a.yg)("strong",{parentName:"mdxAdmonitionTitle"},"Pro Tip")),(0,a.yg)("p",{parentName:"admonition"},"For most industrial applications, ResNet50 provides an excellent balance of accuracy and performance. Only switch to more complex backbones if you have sufficient data and computational resources.")),(0,a.yg)("h3",{id:"backbone-size"},"Backbone Size"),(0,a.yg)("p",null,"The backbone size determines the depth and complexity of the neural network. Larger backbones can learn more complex features but require more memory and computation."),(0,a.yg)("div",{style:{display:"flex",justifyContent:"center"}},(0,a.yg)("table",null,(0,a.yg)("thead",null,(0,a.yg)("tr",null,(0,a.yg)("th",null,"Size Option"),(0,a.yg)("th",null,"Characteristics"),(0,a.yg)("th",null,"Best For"))),(0,a.yg)("tbody",null,(0,a.yg)("tr",null,(0,a.yg)("td",null,"Small"),(0,a.yg)("td",null,"Fewer layers, faster training, lower memory usage"),(0,a.yg)("td",null,"Simple tasks, limited data")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Medium"),(0,a.yg)("td",null,"Balanced depth and complexity"),(0,a.yg)("td",null,"Most general tasks")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Large"),(0,a.yg)("td",null,"More layers, higher capacity"),(0,a.yg)("td",null,"Complex tasks with large datasets")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Extra Large"),(0,a.yg)("td",null,"Deepest networks, highest capacity"),(0,a.yg)("td",null,"Very complex tasks with abundant data"))))),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Quick selection:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"\u2705 Start with ",(0,a.yg)("strong",{parentName:"li"},"Medium")," for most tasks"),(0,a.yg)("li",{parentName:"ul"},"\u2b07\ufe0f Go ",(0,a.yg)("strong",{parentName:"li"},"Small")," if you hit memory errors"),(0,a.yg)("li",{parentName:"ul"},"\u2b06\ufe0f Go ",(0,a.yg)("strong",{parentName:"li"},"Large")," if accuracy plateaus and you have more data")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"How to choose:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"For simple tasks or limited resources: Start with Small or Medium"),(0,a.yg)("li",{parentName:"ul"},"For general tasks with moderate data: Medium or Large"),(0,a.yg)("li",{parentName:"ul"},"For complex tasks with lots of data: Large or Extra Large"),(0,a.yg)("li",{parentName:"ul"},"If you encounter out-of-memory errors during training: Decrease the backbone size")),(0,a.yg)("h3",{id:"output-downsample"},"Output Downsample"),(0,a.yg)("p",null,"The output downsample factor controls the spatial resolution of the model's output feature maps. Lower downsample values retain more spatial detail but increase memory usage."),(0,a.yg)("div",{style:{display:"flex",justifyContent:"center"}},(0,a.yg)("table",null,(0,a.yg)("thead",null,(0,a.yg)("tr",null,(0,a.yg)("th",null,"Downsample Factor"),(0,a.yg)("th",null,"Characteristics"),(0,a.yg)("th",null,"Best For"))),(0,a.yg)("tbody",null,(0,a.yg)("tr",null,(0,a.yg)("td",null,"4x"),(0,a.yg)("td",null,"High resolution output, more detail"),(0,a.yg)("td",null,"Segmentation tasks, small objects")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"8x"),(0,a.yg)("td",null,"Balanced resolution and memory usage"),(0,a.yg)("td",null,"Most tasks, good default choice")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"16x"),(0,a.yg)("td",null,"Lower resolution output, less detail"),(0,a.yg)("td",null,"Large objects, classification tasks"))))),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"How to choose:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"For segmentation or tasks requiring fine detail: Use 4x or 8x"),(0,a.yg)("li",{parentName:"ul"},"For general tasks: 8x is a good starting point"),(0,a.yg)("li",{parentName:"ul"},"For classification or large objects: 16x to save memory"),(0,a.yg)("li",{parentName:"ul"},"If you encounter out-of-memory errors during training: Increase the downsample factor")),(0,a.yg)("h3",{id:"loss-configuration"},"Loss configuration"),(0,a.yg)("p",null,"The loss function measures how far your model's predictions are from the true values, guiding the learning process:"),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Classification Loss Options:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Cross Entropy"),": Standard loss for classification, works well for balanced classes"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Focal Loss"),": Focuses on hard examples, useful when classes are imbalanced"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Binary Cross Entropy"),": Used for multi-label classification where multiple labels can apply")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Segmentation Loss Options:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Dice Loss"),": Measures overlap between predicted and true masks, good for imbalanced pixel classes"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"IoU Loss"),": Based on Intersection over Union, robust to class imbalance"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Cross Entropy"),": Can be used for segmentation but may struggle with class imbalance")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Detection Loss Options:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Combination of regression losses")," (for bounding box coordinates) and classification losses (for object classes)")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"How to choose:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"For balanced classification tasks: Cross Entropy"),(0,a.yg)("li",{parentName:"ul"},"For imbalanced classes (some classes appear much more frequently): Focal Loss"),(0,a.yg)("li",{parentName:"ul"},"For segmentation with varying sized objects: Dice Loss or IoU Loss")),(0,a.yg)("h2",{id:"dataset-parameters"},"Dataset Parameters"),(0,a.yg)("p",null,'Dataset parameters control how your images are processed before and during training, affecting what your model "sees" during the learning process.'),(0,a.yg)("h3",{id:"data-augmentation"},"Data Augmentation"),(0,a.yg)("p",null,"Augmentation artificially expands your training data by applying transformations to create variations of your images. This helps your model generalize better to new, unseen images."),(0,a.yg)("p",null,"Common augmentations include:"),(0,a.yg)("div",{style:{display:"flex",justifyContent:"center"}},(0,a.yg)("table",null,(0,a.yg)("thead",null,(0,a.yg)("tr",null,(0,a.yg)("th",null,"Augmentation"),(0,a.yg)("th",null,"Effect"),(0,a.yg)("th",null,"When to Use"),(0,a.yg)("th",null,"When to Avoid"))),(0,a.yg)("tbody",null,(0,a.yg)("tr",null,(0,a.yg)("td",null,"Horizontal Flip"),(0,a.yg)("td",null,"Mirrors image horizontally"),(0,a.yg)("td",null,"Most applications"),(0,a.yg)("td",null,"When left/right orientation matters (text, asymmetric objects)")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Vertical Flip"),(0,a.yg)("td",null,"Mirrors image vertically"),(0,a.yg)("td",null,"Satellite imagery, microscopy"),(0,a.yg)("td",null,"When up/down orientation matters (most real-world objects)")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Rotation"),(0,a.yg)("td",null,"Rotates image by random angles"),(0,a.yg)("td",null,"Objects that can appear at any orientation"),(0,a.yg)("td",null,"When orientation is fixed in your application")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Translation"),(0,a.yg)("td",null,"Moves image along X or Y axis"),(0,a.yg)("td",null,"Objects that can appear at different positions"),(0,a.yg)("td",null,"When object position is fixed")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Brightness/Contrast/Sharpness"),(0,a.yg)("td",null,"Adjusts lighting conditions"),(0,a.yg)("td",null,"Outdoor applications or varying lighting"),(0,a.yg)("td",null,"High-precision color-based analysis")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Aspect ratio"),(0,a.yg)("td",null,"Changes the aspect ratio of the image"),(0,a.yg)("td",null,"Objects that can appear at different scales"),(0,a.yg)("td",null,"When scale is fixed in your application")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Crop/Pad"),(0,a.yg)("td",null,"Applies cropping or padding"),(0,a.yg)("td",null,"Adding variation for more robust models"),(0,a.yg)("td",null,"Precision measurement applications")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Shear"),(0,a.yg)("td",null,"Applies angular distortion"),(0,a.yg)("td",null,"Adding variation for more robust models"),(0,a.yg)("td",null,"Precision measurement applications")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Color Changes"),(0,a.yg)("td",null,"Varies colors randomly"),(0,a.yg)("td",null,"Natural scene analysis"),(0,a.yg)("td",null,"Medical imaging, color-critical applications")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Background Changes"),(0,a.yg)("td",null,"Varies background colors or patterns"),(0,a.yg)("td",null,"Helps models to focus on foreground objects"),(0,a.yg)("td",null,"Background-foreground separation tasks")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Distractors addition"),(0,a.yg)("td",null,"Adds irrelevant objects or patterns"),(0,a.yg)("td",null,"Improves model robustness to distractions"),(0,a.yg)("td",null,"When focusing on specific objects"))))),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Recommended configurations:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"For ",(0,a.yg)("strong",{parentName:"li"},"general classification"),": Enable horizontal flip, minor rotation (\xb115\xb0), brightness/contrast adjustments"),(0,a.yg)("li",{parentName:"ul"},"For ",(0,a.yg)("strong",{parentName:"li"},"object detection"),": All of the above plus minor scaling/zoom variations"),(0,a.yg)("li",{parentName:"ul"},"For ",(0,a.yg)("strong",{parentName:"li"},"precise segmentation"),": Limited augmentations - horizontal flip and subtle brightness adjustments"),(0,a.yg)("li",{parentName:"ul"},"For ",(0,a.yg)("strong",{parentName:"li"},"outdoor scenes"),": More aggressive brightness/contrast and weather augmentations")),(0,a.yg)("admonition",{type:"caution"},(0,a.yg)("mdxAdmonitionTitle",{parentName:"admonition"},(0,a.yg)("strong",{parentName:"mdxAdmonitionTitle"},"Important")),(0,a.yg)("p",{parentName:"admonition"},"Choose augmentations that reflect realistic variations your model will encounter in production. Unrealistic augmentations can harm performance.")),(0,a.yg)("h2",{id:"training-parameters"},"Training Parameters"),(0,a.yg)("p",null,"These settings control how your model learns from data during the training process."),(0,a.yg)("h3",{id:"learning-rate"},"Learning rate"),(0,a.yg)("p",null,"The most critical training parameter - controls how quickly your model adapts to errors."),(0,a.yg)("figure",null,(0,a.yg)("img",{src:n(8288).A,alt:"Learning rate effects"}),(0,a.yg)("figcaption",null,"Effects of different learning rates on training convergence.")),(0,a.yg)("div",{style:{display:"flex",justifyContent:"center"}},(0,a.yg)("table",null,(0,a.yg)("thead",null,(0,a.yg)("tr",null,(0,a.yg)("th",null,"Rate"),(0,a.yg)("th",null,"Effect"),(0,a.yg)("th",null,"When It Happens"))),(0,a.yg)("tbody",null,(0,a.yg)("tr",null,(0,a.yg)("td",null,"Too High (>0.1)"),(0,a.yg)("td",null,"Training explodes or oscillates wildly"),(0,a.yg)("td",null,"Loss increases or jumps erratically")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Good (0.001-0.01)"),(0,a.yg)("td",null,"Steady improvement, stable training"),(0,a.yg)("td",null,"Loss decreases smoothly")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Too Low (<0.0001)"),(0,a.yg)("td",null,"Very slow progress, may get stuck"),(0,a.yg)("td",null,"Training barely improves"))))),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Recommended starting values:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"SGD optimizer"),": 0.01 - 0.1"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Adam optimizer"),": 0.0001 - 0.001"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Fine-tuning pretrained"),": 0.0001 - 0.00001")),(0,a.yg)("admonition",{title:"Tuning Strategy",type:"tip"},(0,a.yg)("p",{parentName:"admonition"},"Start with the default. If training is unstable (loss spikes), reduce by 10x. If progress is too slow after 20 epochs, increase by 2-3x.")),(0,a.yg)("h4",{id:"batch-size"},"Batch Size"),(0,a.yg)("p",null,"Number of images processed together in each training step."),(0,a.yg)("div",{style:{display:"flex",justifyContent:"center"}},(0,a.yg)("table",null,(0,a.yg)("thead",null,(0,a.yg)("tr",null,(0,a.yg)("th",null,"Batch Size"),(0,a.yg)("th",null,"Memory"),(0,a.yg)("th",null,"Speed"),(0,a.yg)("th",null,"Stability"),(0,a.yg)("th",null,"When to Use"))),(0,a.yg)("tbody",null,(0,a.yg)("tr",null,(0,a.yg)("td",null,"4"),(0,a.yg)("td",null,"Low"),(0,a.yg)("td",null,"Slower"),(0,a.yg)("td",null,"Less stable"),(0,a.yg)("td",null,"Memory-limited GPUs")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"8-16"),(0,a.yg)("td",null,"Medium"),(0,a.yg)("td",null,(0,a.yg)("strong",null,"Optimal")),(0,a.yg)("td",null,"Good"),(0,a.yg)("td",null,(0,a.yg)("strong",null,"Recommended default"))),(0,a.yg)("tr",null,(0,a.yg)("td",null,"32+"),(0,a.yg)("td",null,"High"),(0,a.yg)("td",null,"Faster"),(0,a.yg)("td",null,"Very stable"),(0,a.yg)("td",null,"Large GPUs, simple tasks"))))),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Rule of thumb:")," Use the largest batch size that fits in your GPU memory."),(0,a.yg)("h4",{id:"epochs"},"Epochs"),(0,a.yg)("p",null,"Complete passes through your entire training dataset."),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Max Epochs"),": Upper limit on training duration"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Too few (< 50): Model doesn't learn enough (underfitting)"),(0,a.yg)("li",{parentName:"ul"},"Too many (> 500): Wasted computation"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Recommended"),": 100-200 epochs with early stopping")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Min Epochs"),": Minimum training duration before stopping is allowed"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Prevents premature stopping"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Recommended"),": 30-50 epochs")),(0,a.yg)("admonition",{title:"Best Practice",type:"tip"},(0,a.yg)("p",{parentName:"admonition"},"Set Max Epochs to a generous number (e.g., 200) and rely on ",(0,a.yg)("strong",{parentName:"p"},"early stopping")," to halt training at the optimal point.")),(0,a.yg)("h4",{id:"early-stopping"},"Early Stopping"),(0,a.yg)("p",null,"Automatically stops training when performance plateaus, preventing overfitting and saving time."),(0,a.yg)("figure",null,(0,a.yg)("img",{src:n(8288).A,alt:"Early stopping"}),(0,a.yg)("figcaption",null,"Early stopping prevents overfitting by halting training when validation metrics plateau.")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Key settings:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Patience"),": Epochs to wait without improvement (typical: 10-15)"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Metric"),": What to monitor (validation loss, accuracy, IoU)"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Mode"),": Maximize (accuracy) or minimize (loss)")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Example:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"Patience: 15 epochs\nMetric: Validation Loss\nMode: Minimize\n\n\u2192 Training stops if loss doesn't improve for 15 consecutive epochs\n")),(0,a.yg)("h3",{id:"additional-training-settings"},"Additional Training Settings"),(0,a.yg)("h4",{id:"class-balancing"},"Class Balancing"),(0,a.yg)("p",null,"Adjusts training to handle imbalanced datasets where some classes appear much more frequently than others."),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Enable when:")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"\u2705 One class has 3x+ more examples than others"),(0,a.yg)("li",{parentName:"ul"},"\u2705 Rare defect detection (95% good, 5% defective)"),(0,a.yg)("li",{parentName:"ul"},"\u2705 Model ignores minority classes")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"How it helps:")," Prevents model from achieving high accuracy by simply predicting the majority class."),(0,a.yg)("h4",{id:"mixed-precision-training"},"Mixed Precision Training"),(0,a.yg)("p",null,"Uses 16-bit floats instead of 32-bit for faster training with lower memory usage."),(0,a.yg)("div",{style:{display:"flex",justifyContent:"center"}},(0,a.yg)("table",null,(0,a.yg)("thead",null,(0,a.yg)("tr",null,(0,a.yg)("th",null,"Aspect"),(0,a.yg)("th",null,"Benefit"),(0,a.yg)("th",null,"Trade-off"))),(0,a.yg)("tbody",null,(0,a.yg)("tr",null,(0,a.yg)("td",null,"Speed"),(0,a.yg)("td",null,"2-3x faster"),(0,a.yg)("td",null,"Slightly reduced numerical precision")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Memory"),(0,a.yg)("td",null,"~50% reduction"),(0,a.yg)("td",null,"Rare numerical instability")),(0,a.yg)("tr",null,(0,a.yg)("td",null,"Accuracy"),(0,a.yg)("td",null,"Negligible impact"),(0,a.yg)("td",null,"May need adjustments in extreme cases"))))),(0,a.yg)("admonition",{title:"Recommendation",type:"tip"},(0,a.yg)("p",{parentName:"admonition"},(0,a.yg)("strong",{parentName:"p"},"Keep enabled by default.")," Only disable if you notice training instability or numerical issues (very rare).")))}d.isMDXComponent=!0}}]);