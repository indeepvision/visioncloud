"use strict";(self.webpackChunk_indeepvision_visioncloud_docs=self.webpackChunk_indeepvision_visioncloud_docs||[]).push([[365],{680:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>d,frontMatter:()=>l,metadata:()=>o,toc:()=>g});var a=n(9575),i=(n(4041),n(2247));const l={sidebar_position:6},r="Best Practices",o={unversionedId:"user-guide/models/best-practices",id:"user-guide/models/best-practices",title:"Best Practices",description:"Follow these guidelines to optimize your model training process and achieve better performance with less computational cost.",source:"@site/docs/user-guide/models/best-practices.md",sourceDirName:"user-guide/models",slug:"/user-guide/models/best-practices",permalink:"/visioncloud/docs/user-guide/models/best-practices",draft:!1,tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"userGuide",previous:{title:"Download Model",permalink:"/visioncloud/docs/user-guide/models/download-model"},next:{title:"Troubleshooting Guide",permalink:"/visioncloud/docs/user-guide/models/troubleshooting"}},s={},g=[{value:"Model type",id:"model-type",level:2},{value:"Classification vs. Segmentation",id:"classification-vs-segmentation",level:3},{value:"Why Segmentation Often Wins",id:"why-segmentation-often-wins",level:4},{value:"Model Input",id:"model-input",level:2},{value:"When to Use Patch Inference",id:"when-to-use-patch-inference",level:3},{value:"Use patch inference when:",id:"use-patch-inference-when",level:4},{value:"Example scenarios:",id:"example-scenarios",level:4},{value:"Using Regions of Interest (ROIs)",id:"using-regions-of-interest-rois",level:3},{value:"Why use ROIs?",id:"why-use-rois",level:4},{value:"When to define ROIs:",id:"when-to-define-rois",level:4},{value:"Example use cases:",id:"example-use-cases",level:4},{value:"Model Architecture",id:"model-architecture",level:2},{value:"Right-Sizing Your Model",id:"right-sizing-your-model",level:3},{value:"The Model Size Trap",id:"the-model-size-trap",level:4},{value:"Dataset augmentation",id:"dataset-augmentation",level:2},{value:"Preserve Label Validity",id:"preserve-label-validity",level:3},{value:"Common Mistakes to Avoid",id:"common-mistakes-to-avoid",level:4},{value:"Augmentation Decision Matrix",id:"augmentation-decision-matrix",level:3},{value:"Before Enabling Augmentation: Checklist",id:"before-enabling-augmentation-checklist",level:3},{value:"Quick Best Practices Summary",id:"quick-best-practices-summary",level:2},{value:"Related Resources",id:"related-resources",level:2}],c={toc:g},u="wrapper";function d(e){let{components:t,...n}=e;return(0,i.yg)(u,(0,a.A)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"best-practices"},"Best Practices"),(0,i.yg)("p",null,"Follow these guidelines to optimize your model training process and achieve better performance with less computational cost."),(0,i.yg)("h2",{id:"model-type"},"Model type"),(0,i.yg)("h3",{id:"classification-vs-segmentation"},"Classification vs. Segmentation"),(0,i.yg)("p",null,"Understanding when to use each approach is crucial for model performance."),(0,i.yg)("h4",{id:"why-segmentation-often-wins"},"Why Segmentation Often Wins"),(0,i.yg)("p",null,"In most cases, ",(0,i.yg)("strong",{parentName:"p"},"segmentation is preferable to classification"),", since it provides greater robustness."),(0,i.yg)("table",null,(0,i.yg)("thead",{parentName:"table"},(0,i.yg)("tr",{parentName:"thead"},(0,i.yg)("th",{parentName:"tr",align:null},"Approach"),(0,i.yg)("th",{parentName:"tr",align:null},"Information Used"),(0,i.yg)("th",{parentName:"tr",align:null},"Best For"))),(0,i.yg)("tbody",{parentName:"table"},(0,i.yg)("tr",{parentName:"tbody"},(0,i.yg)("td",{parentName:"tr",align:null},(0,i.yg)("strong",{parentName:"td"},"Segmentation")),(0,i.yg)("td",{parentName:"tr",align:null},"Geometric + color + spatial data"),(0,i.yg)("td",{parentName:"tr",align:null},"Precise localization, irregular objects")),(0,i.yg)("tr",{parentName:"tbody"},(0,i.yg)("td",{parentName:"tr",align:null},(0,i.yg)("strong",{parentName:"td"},"Classification")),(0,i.yg)("td",{parentName:"tr",align:null},"Category assignment only"),(0,i.yg)("td",{parentName:"tr",align:null},"Simple presence/absence detection")))),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Key advantage:")," Segmentation leverages geometric and color-based information, while classification relies solely on category assignment without structural detail."),(0,i.yg)("h2",{id:"model-input"},"Model Input"),(0,i.yg)("h3",{id:"when-to-use-patch-inference"},"When to Use Patch Inference"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Patch inference")," divides large images into smaller patches for processing, then combines the results."),(0,i.yg)("h4",{id:"use-patch-inference-when"},"Use patch inference when:"),(0,i.yg)("p",null,"\u2705 Analyzing large images in high resolution",(0,i.yg)("br",{parentName:"p"}),"\n","\u2705 Your image resolution exceeds your model's input size",(0,i.yg)("br",{parentName:"p"}),"\n","\u2705 You need to preserve fine details in large scenes",(0,i.yg)("br",{parentName:"p"}),"\n","\u2705 Memory constraints prevent processing the full image  "),(0,i.yg)("h4",{id:"example-scenarios"},"Example scenarios:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"High-resolution satellite imagery (4000x4000+)"),(0,i.yg)("li",{parentName:"ul"},"Whole-slide medical imaging"),(0,i.yg)("li",{parentName:"ul"},"Detailed industrial inspection of large surfaces"),(0,i.yg)("li",{parentName:"ul"},"Panoramic scene analysis")),(0,i.yg)("admonition",{title:"Performance Benefit",type:"tip"},(0,i.yg)("p",{parentName:"admonition"},"Patch inference allows you to work with high-resolution images without sacrificing detail or overwhelming your GPU memory.")),(0,i.yg)("h3",{id:"using-regions-of-interest-rois"},"Using Regions of Interest (ROIs)"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"ROIs")," allow you to focus model attention on specific areas of your images where analysis is needed."),(0,i.yg)("h4",{id:"why-use-rois"},"Why use ROIs?"),(0,i.yg)("div",{style:{display:"flex",justifyContent:"center"}},(0,i.yg)("table",null,(0,i.yg)("thead",null,(0,i.yg)("tr",null,(0,i.yg)("th",null,"Benefit"),(0,i.yg)("th",null,"Impact"))),(0,i.yg)("tbody",null,(0,i.yg)("tr",null,(0,i.yg)("td",null,"Focused attention"),(0,i.yg)("td",null,"Model learns relevant features, ignores background noise")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"Faster training"),(0,i.yg)("td",null,"Smaller input size = faster processing")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"Faster inference"),(0,i.yg)("td",null,"Less data to process in production")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"Better accuracy"),(0,i.yg)("td",null,"Model isn't distracted by irrelevant areas")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"Memory efficiency"),(0,i.yg)("td",null,"Process only what matters"))))),(0,i.yg)("h4",{id:"when-to-define-rois"},"When to define ROIs:"),(0,i.yg)("p",null,"\u2705 ",(0,i.yg)("strong",{parentName:"p"},"Use ROIs when:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Your area of interest is a small portion of the full image"),(0,i.yg)("li",{parentName:"ul"},"Background contains irrelevant or distracting information"),(0,i.yg)("li",{parentName:"ul"},"Objects always appear in predictable locations"),(0,i.yg)("li",{parentName:"ul"},"You want to exclude certain areas from analysis")),(0,i.yg)("p",null,"\u274c ",(0,i.yg)("strong",{parentName:"p"},"Skip ROIs when:")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"The entire image is relevant"),(0,i.yg)("li",{parentName:"ul"},"Object locations vary unpredictably across the full frame"),(0,i.yg)("li",{parentName:"ul"},"You need context from the full scene")),(0,i.yg)("h4",{id:"example-use-cases"},"Example use cases:"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Manufacturing line inspection:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre"},"Full image: 1920x1080 (entire conveyor belt)\nROI: 800x600 (product inspection area only)\nResult: 3x faster inference, better defect detection\n")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Document analysis:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre"},"Full image: 2480x3508 (entire page)\nROI: 2000x400 (header section only)\nResult: Focus on relevant text, ignore margins\n")),(0,i.yg)("admonition",{title:"Pro Tip",type:"tip"},(0,i.yg)("p",{parentName:"admonition"},"Define ROIs during the dataset creation phase. This ensures consistent preprocessing and optimal model training.")),(0,i.yg)("h2",{id:"model-architecture"},"Model Architecture"),(0,i.yg)("h3",{id:"right-sizing-your-model"},"Right-Sizing Your Model"),(0,i.yg)("p",null,"Bigger isn't always better - choose model size based on your actual needs."),(0,i.yg)("h4",{id:"the-model-size-trap"},"The Model Size Trap"),(0,i.yg)("p",null,"Even if your original images are large, this doesn't automatically mean you need a large model."),(0,i.yg)("div",{style:{display:"flex",justifyContent:"center"}},(0,i.yg)("table",null,(0,i.yg)("thead",null,(0,i.yg)("tr",null,(0,i.yg)("th",null,"Scenario"),(0,i.yg)("th",null,"Image Size"),(0,i.yg)("th",null,"Recommended Model Size"))),(0,i.yg)("tbody",null,(0,i.yg)("tr",null,(0,i.yg)("td",null,"Simple objects, clear features"),(0,i.yg)("td",null,"Large (2000x2000)"),(0,i.yg)("td",null,"Small to Medium")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"Complex patterns, fine details"),(0,i.yg)("td",null,"Large (2000x2000)"),(0,i.yg)("td",null,"Medium to Large")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"Simple task on small objects"),(0,i.yg)("td",null,"Small (512x512)"),(0,i.yg)("td",null,"Small")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"Complex segmentation"),(0,i.yg)("td",null,"Medium (1024x1024)"),(0,i.yg)("td",null,"Medium to Large"))))),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Why avoid oversized models?")),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"\u274c Longer training times"),(0,i.yg)("li",{parentName:"ul"},"\u274c Higher memory requirements"),(0,i.yg)("li",{parentName:"ul"},"\u274c Risk of overfitting on small datasets"),(0,i.yg)("li",{parentName:"ul"},"\u274c Slower inference in production")),(0,i.yg)("admonition",{title:"Remember",type:"caution"},(0,i.yg)("p",{parentName:"admonition"},"Start with a smaller model and scale up only if performance plateaus. A well-trained medium model often outperforms a poorly trained large one.")),(0,i.yg)("h2",{id:"dataset-augmentation"},"Dataset augmentation"),(0,i.yg)("p",null,"Augmentation is powerful, but incorrect augmentation can destroy your labels and harm model performance."),(0,i.yg)("h3",{id:"preserve-label-validity"},"Preserve Label Validity"),(0,i.yg)("p",null,"Your augmentations must not invalidate your labels. Always ask: ",(0,i.yg)("strong",{parentName:"p"},'"Does this transformation change what the label represents?"')),(0,i.yg)("h4",{id:"common-mistakes-to-avoid"},"Common Mistakes to Avoid"),(0,i.yg)("p",null,"Example 1: Orientation Classification"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre"},'Task: Classify bottle orientation (upright vs. horizontal)\n\nWrong approach:\n\nAugmentations enabled:\n\u2705 Brightness adjustment\n\u274c Horizontal flip  \u2190 This changes the orientation!\n\u274c Vertical flip    \u2190 This changes the orientation!\n\u274c Rotation         \u2190 This changes the orientation!\n\n\nWhy it\'s wrong:These augmentations alter the very feature you\'re trying to classify. A flipped "upright" bottle becomes "inverted" but keeps the "upright" label.\n\nCorrect approach:\n\nAugmentations enabled:\n\u2705 Brightness adjustment\n\u2705 Contrast adjustment  \n\u2705 Minor scale changes\n\u274c Any rotation or flipping\n')),(0,i.yg)("p",null,"Example 2: Text Direction Detection"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre"},"Task: Detect text reading direction (left-to-right vs. right-to-left)\n\nWrong approach:\n\n\u274c Horizontal flip \u2190 Reverses text direction!\n\n\nCorrect approach:\n\n\u2705 Brightness/contrast only\n\u2705 Slight scale variations\n\u274c No geometric transformations\n")),(0,i.yg)("p",null,"Example 3: Defect Location Matters"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre"},"Task: Detect defects on specific product sides (front, back, left, right)\n\nWrong approach:\n\n\u274c Any flipping or rotation\n(Changes which side is which)\n\n\nCorrect approach:\n\n\u2705 Lighting variations\n\u2705 Slight perspective changes\n\u274c No flips or rotations\n\n")),(0,i.yg)("h3",{id:"augmentation-decision-matrix"},"Augmentation Decision Matrix"),(0,i.yg)("p",null,"Use this guide to decide which augmentations are safe:"),(0,i.yg)("div",{style:{display:"flex",justifyContent:"center"}},(0,i.yg)("table",null,(0,i.yg)("thead",null,(0,i.yg)("tr",null,(0,i.yg)("th",null,"Your Task"),(0,i.yg)("th",null,"Safe Augmentations"),(0,i.yg)("th",null,"Dangerous Augmentations"))),(0,i.yg)("tbody",null,(0,i.yg)("tr",null,(0,i.yg)("td",null,"Orientation/direction classification"),(0,i.yg)("td",null,"Brightness, contrast, color, noise"),(0,i.yg)("td",null,"Rotation, flips")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"General object detection"),(0,i.yg)("td",null,"All geometric + photometric"),(0,i.yg)("td",null,"None (usually)")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"Symmetrical object detection"),(0,i.yg)("td",null,"All augmentations"),(0,i.yg)("td",null,"Check if asymmetric features matter")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"Position-dependent classification"),(0,i.yg)("td",null,"Brightness, contrast only"),(0,i.yg)("td",null,"Any geometric transform")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"Color-based classification"),(0,i.yg)("td",null,"Geometric only"),(0,i.yg)("td",null,"Color jitter, brightness, contrast")),(0,i.yg)("tr",null,(0,i.yg)("td",null,"Size-based classification"),(0,i.yg)("td",null,"Most except scale"),(0,i.yg)("td",null,"Zoom, scale augmentations"))))),(0,i.yg)("h3",{id:"before-enabling-augmentation-checklist"},"Before Enabling Augmentation: Checklist"),(0,i.yg)("p",null,"Ask yourself these questions:"),(0,i.yg)("ul",{className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Does my label depend on orientation? \u2192 Avoid rotation/flips"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Does my label depend on position? \u2192 Avoid translations"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Does my label depend on size? \u2192 Avoid scaling"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Does my label depend on color? \u2192 Avoid color augmentations"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Does my label depend on direction? \u2192 Avoid flips"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Am I classifying based on geometry? \u2192 Avoid geometric augmentations")),(0,i.yg)("admonition",{title:"Critical",type:"danger"},(0,i.yg)("p",{parentName:"admonition"},"When in doubt, test your augmentation pipeline. Visually inspect augmented images with their labels overlaid. If the label no longer matches the transformed image, disable that augmentation.")),(0,i.yg)("h2",{id:"quick-best-practices-summary"},"Quick Best Practices Summary"),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Before training:")),(0,i.yg)("ul",{className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Use patch inference for high-resolution images (>2000px)"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Choose model size based on task complexity, not just image size"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Define ROIs to focus on relevant areas"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Test augmentations don't invalidate labels"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Start with conservative augmentation settings")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"During training:")),(0,i.yg)("ul",{className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Monitor that augmented samples still make sense"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Verify ROIs capture all relevant information"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Adjust patch size if inference is slow")),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"After training:")),(0,i.yg)("ul",{className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Validate model on non-augmented test data"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Check performance on full images vs. patches"),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Verify ROI-based predictions match expectations")),(0,i.yg)("hr",null),(0,i.yg)("h2",{id:"related-resources"},"Related Resources"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("a",{parentName:"li",href:"./advanced-configuration"},"Advanced Configuration")," - Detailed parameter tuning guide"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("a",{parentName:"li",href:"./advanced-configuration#data-augmentation"},"Data Augmentation Reference")," - Complete augmentation catalog"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("a",{parentName:"li",href:"./troubleshooting"},"Troubleshooting Guide")," - Common training issues and solutions")))}d.isMDXComponent=!0},2247:(e,t,n)=>{n.d(t,{xA:()=>c,yg:()=>p});var a=n(4041);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=a.createContext({}),g=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=g(e.components);return a.createElement(s.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,l=e.originalType,s=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),u=g(n),m=i,p=u["".concat(s,".").concat(m)]||u[m]||d[m]||l;return n?a.createElement(p,r(r({ref:t},c),{},{components:n})):a.createElement(p,r({ref:t},c))}));function p(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var l=n.length,r=new Array(l);r[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[u]="string"==typeof e?e:i,r[1]=o;for(var g=2;g<l;g++)r[g]=n[g];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"}}]);